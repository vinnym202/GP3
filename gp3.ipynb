{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3rwy7V72-8YY"
   },
   "source": [
    "# GP3 Trading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Z6qlLXY-fA2"
   },
   "outputs": [],
   "source": [
    "from gp3.config_private import ALPACA_API_KEY, ALPACA_API_SECRET\n",
    "API_BASE_URL = 'https://paper-api.alpaca.markets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7I7zsyYfoLJ",
    "outputId": "1812c13b-410f-434c-eb07-29782ba186e6"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "sys.path.append('gp3')\n",
    "from gp3.config import INDICATORS, CDL, TRAIN_START_DATE, TRAIN_END_DATE\n",
    "from gp3.config import TEST_START_DATE, TEST_END_DATE\n",
    "from gp3.config_tickers import GP3_TICKERS\n",
    "from gp3.data_processor import DataProcessor\n",
    "\n",
    "from gp3.elegantRL.env.StockTradingEnv import StockTradingEnv\n",
    "from gp3.elegantRL.agent.AgentPPO import AgentPPO\n",
    "from gp3.elegantRL.agent.DRLAgent import DRLAgent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIV0kO_y-inG",
    "outputId": "bd7b3c21-641e-4eb7-a4af-ae7d156042a6"
   },
   "outputs": [],
   "source": [
    "print(GP3_TICKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CnqQ-cC5-rfO",
    "outputId": "29b248c9-ec98-44cd-befb-65192af72ea4"
   },
   "outputs": [],
   "source": [
    "print(INDICATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StockTradingEnv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Create 'train_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"train_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'train_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array = dp.df_to_array(data, if_vix)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array), f)\n",
    "\n",
    "    # read parameters\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "    \n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        break_step = kwargs.get(\"break_step\", 1e6)\n",
    "        erl_params = kwargs.get(\"erl_params\")\n",
    "        agent = DRLAgent_erl(\n",
    "            env=env,\n",
    "            price_array=price_array,\n",
    "            tech_array=tech_array,\n",
    "            turbulence_array=turbulence_array,\n",
    "        )\n",
    "        model = agent.get_model(model_name, model_kwargs=erl_params)\n",
    "        agent.train_model(\n",
    "            model=model, cwd=cwd, total_timesteps=break_step\n",
    "        )\n",
    "\n",
    "def test(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Create 'test_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"test_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'test_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array = dp.df_to_array(data, if_vix)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array), f)\n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"if_train\": False,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "\n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        episode_total_assets, max_drawdown, avg_reward  = DRLAgent_erl.DRL_prediction(\n",
    "            env=env_instance,\n",
    "            cwd=cwd,\n",
    "        )\n",
    "        \n",
    "        return episode_total_assets, max_drawdown, avg_reward\n",
    "\n",
    "def update_leaderboard(cwd, leaderboard_dir, episode_total_assets, max_drawdown, avg_reward, erl_params):\n",
    "    actor_folder_name = f\"actor_{round(episode_total_assets)}\"\n",
    "    leaderboard_path = os.path.join(leaderboard_dir, actor_folder_name)\n",
    "\n",
    "    # Move and rename the directory\n",
    "    if not os.path.exists(leaderboard_path):\n",
    "        shutil.move(cwd, leaderboard_path)\n",
    "    else:\n",
    "        shutil.rmtree(cwd, ignore_errors=True)\n",
    "    \n",
    "    time.sleep(0.02)\n",
    "        \n",
    "    leaderboard_data_path = os.path.join(leaderboard_dir, 'leaderboard_data.json')\n",
    "\n",
    "    # Load the leaderboard data if it exists\n",
    "    if os.path.isfile(leaderboard_data_path):\n",
    "        with open(leaderboard_data_path, 'r') as f:\n",
    "            leaderboard_data = json.load(f)\n",
    "    else:\n",
    "        leaderboard_data = []\n",
    "\n",
    "    # Add the new trial data\n",
    "    new_entry = {\n",
    "        'episode_total_assets': float(episode_total_assets),\n",
    "        'max_drawdown': float(max_drawdown),\n",
    "        'avg_reward': float(avg_reward),\n",
    "        'hyperparameters': erl_params,\n",
    "        'model_dir': leaderboard_path,\n",
    "    }\n",
    "\n",
    "    leaderboard_data.append(new_entry)\n",
    "\n",
    "    # Sort the leaderboard data based on episode_total_assets\n",
    "    leaderboard_data.sort(key=lambda x: x['episode_total_assets'], reverse=True)\n",
    "\n",
    "    # Keep only the top 10 models\n",
    "    leaderboard_data = leaderboard_data[:10]\n",
    "\n",
    "    # Delete the models outside of the top 10\n",
    "    best_models_dirs = {entry['model_dir'] for entry in leaderboard_data}\n",
    "    for model_dir in os.listdir(leaderboard_dir):\n",
    "        full_model_dir_path = os.path.join(leaderboard_dir, model_dir)\n",
    "        if full_model_dir_path not in best_models_dirs and os.path.isdir(full_model_dir_path):\n",
    "            shutil.rmtree(full_model_dir_path)\n",
    "    time.sleep(0.02)\n",
    "    # Save the updated leaderboard data\n",
    "    with open(leaderboard_data_path, 'w') as f:\n",
    "        json.dump(leaderboard_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time = (3600 * 1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    learning_rate = random.uniform(1e-6, 1e-3)\n",
    "    weight_decay = random.uniform(1e-5, 1e-3)\n",
    "    lambda_entropy = random.uniform(1e-2, 5e-2)\n",
    "    lambda_gae_adv = round(random.uniform(0.5, 0.99), 2)\n",
    "    clip_grad_norm = 3.0\n",
    "    ratio_clip = 0.25\n",
    "    batch_size = 256\n",
    "    gamma = 0.995\n",
    "    net_dimension = [256, 128, 1024]\n",
    "    horizon_len = 1024\n",
    "    repeat_times = 16\n",
    "    optimizer = \"RAdam\"\n",
    "    activation = \"GELU\"\n",
    "    reward_scaling = 2 ** -11\n",
    "    random_seed = 42\n",
    "\n",
    "    erl_params = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"lambda_entropy\": lambda_entropy,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gamma\": gamma,\n",
    "        \"net_dimension\": net_dimension,\n",
    "        \"horizon_len\": horizon_len,\n",
    "        \"repeat_times\": repeat_times,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"clip_grad_norm\": clip_grad_norm,\n",
    "        \"ratio_clip\": ratio_clip,\n",
    "        \"lambda_gae_adv\": lambda_gae_adv,\n",
    "        \"activation\": activation,\n",
    "        \"reward_scaling\": reward_scaling,\n",
    "        \"random_seed\": random_seed,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        train(\n",
    "            start_date = TRAIN_START_DATE, \n",
    "            end_date = TRAIN_END_DATE,\n",
    "            ticker_list = GP3_TICKERS, \n",
    "            data_source = 'alpaca',\n",
    "            time_interval = '1D', \n",
    "            technical_indicator_list = INDICATORS,\n",
    "            drl_lib ='elegantrl', \n",
    "            env = env,\n",
    "            model_name = \"ppo\",\n",
    "            if_vix = True,\n",
    "            API_KEY = ALPACA_API_KEY, \n",
    "            API_SECRET = ALPACA_API_SECRET, \n",
    "            API_BASE_URL = API_BASE_URL,\n",
    "            cwd = './gp3_testing', #current_working_dir\n",
    "            break_step = 1e5,\n",
    "            erl_params = erl_params,\n",
    "        )\n",
    "\n",
    "        episode_total_assets, max_drawdown, avg_reward = test(\n",
    "            start_date = TEST_START_DATE, \n",
    "            end_date = TEST_END_DATE,\n",
    "            ticker_list = GP3_TICKERS, \n",
    "            data_source = 'alpaca',\n",
    "            time_interval = '1D', \n",
    "            technical_indicator_list = INDICATORS,\n",
    "            drl_lib = 'elegantrl', \n",
    "            env = env, \n",
    "            model_name = \"ppo\",\n",
    "            if_vix = True,\n",
    "            API_KEY = ALPACA_API_KEY, \n",
    "            API_SECRET = ALPACA_API_SECRET, \n",
    "            API_BASE_URL = API_BASE_URL,\n",
    "            cwd = './gp3_testing',\n",
    "        )\n",
    "\n",
    "        update_leaderboard('./gp3_testing', './gp3_leaderboard', episode_total_assets, max_drawdown, avg_reward, erl_params)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error occurred: {e}')\n",
    "        continue\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > run_time:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0EVJIQUR6_fu",
    "9tzAw9k26nAC",
    "zjLda8No6pvI",
    "pf5aVHAU-xF6",
    "rZMkcyjZ-25l",
    "3rwy7V72-8YY",
    "J25MuZLiGqCP",
    "eW0UDAXI1nEa",
    "UFoxkigg1zXa"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "005d14239094016f48a03a57365c4ccb734e3f38c20ed0ca595d84f773bc39cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
